{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import sqlite3\n",
    "\n",
    "TIKI_URL = 'https://tiki.vn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('tiki.db')\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute('DROP TABLE categories;')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the HTML content get_url()\n",
    "def get_url(url):\n",
    "    try:\n",
    "        response = requests.get(url).text\n",
    "        soup = BeautifulSoup(response, 'html.parser')\n",
    "        return soup\n",
    "    except Exception as err:\n",
    "        print('ERROR BY REQUEST:', err)\n",
    "    except "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table categories in the database using a function\n",
    "def create_categories_table():\n",
    "    query = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS categories (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            name VARCHAR(255),\n",
    "            url TEXT, \n",
    "            parent_id INTEGER,\n",
    "            create_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        )\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cur.execute(query) # execute the query\n",
    "        conn.commit() # make changes\n",
    "    except Exception as err: \n",
    "        print('ERROR BY CREATE TABLE', err)\n",
    "        \n",
    "create_categories_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute('SELECT * FROM categories').fetchall() # there you go, table was created, as good as new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to add cats to the table, but wait, let's create some cool OOP atributes and methods first\n",
    "\n",
    "class Category:\n",
    "    \n",
    "    # create some atributes\n",
    "    def __init__(self, name, url, parent_id = None,have_child = 1, cat_id= None):\n",
    "        self.name = name\n",
    "        self.url = url\n",
    "        self.parent_id = parent_id\n",
    "        self.cat_id = cat_id\n",
    "        self.have_child = have_child\n",
    "    \n",
    "    # how to display this cat when being called\n",
    "    def __repr__(self):\n",
    "        return f\"ID: {self.cat_id}, Name: {self.name}, URL: {self.url}, Parent: {self.parent_id} \"\n",
    "    \n",
    "    # so many things are going on here, let's dive in\n",
    "    def save_into_db(self):\n",
    "        query = \"\"\"\n",
    "            INSERT INTO categories (name, url, parent_id)\n",
    "            VALUES (?, ?, ?);\n",
    "        \"\"\"\n",
    "        val = (self.name, self.url, self.parent_id)\n",
    "        try:\n",
    "            cur.execute(query, val)\n",
    "            self.cat_id = cur.lastrowid # this creates the cat_id, it assigns that cat_id atribute to lastrowid, so it changes the repr, woohoo\n",
    "            conn.commit()\n",
    "        except Exception as err:\n",
    "            print('ERROR BY INSERT:', err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's scrape the main cats:\n",
    "def get_main_cats(save_db = False):\n",
    "    \n",
    "    # get our soup from the pre-defined get_url function\n",
    "    soup = get_url(TIKI_URL)\n",
    "    \n",
    "    main_cats = []\n",
    "    \n",
    "    for a in soup.find_all('a',class_=\"MenuItem__MenuLink-sc-181aa19-1 fKvTQu\"):\n",
    "        name = a.find('span', class_='text').text\n",
    "        url = a['href']\n",
    "        cat = Category(name, url)\n",
    "        main_cats.append(cat)\n",
    "        \n",
    "        \n",
    "        if save_db:\n",
    "            cat.save_into_db()\n",
    "        \n",
    "    return main_cats\n",
    "        \n",
    "    \n",
    "main_cats = get_main_cats(save_db = True) \n",
    "main_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re is our good cleaner to tidy things up\n",
    "import re\n",
    "\n",
    "\n",
    "# get_sub_categories() given a parent category\n",
    "def get_sub_categories(parent_category, save_db = False):\n",
    "    parent_url = parent_category.url\n",
    "    result = []\n",
    "    \n",
    "    try:\n",
    "        soup = get_url(parent_url)\n",
    "        for div in soup.find_all('div',class_='list-group-item is-child'):\n",
    "            name = div.a.text\n",
    "        \n",
    "            # here comes our cleaner\n",
    "            name = re.sub('\\s+|\\\\n', ' ', name)\n",
    "            \n",
    "            sub_url = TIKI_URL + div.a['href']\n",
    "            \n",
    "            cat = Category(name, sub_url, parent_category.cat_id)\n",
    "            result.append(cat)\n",
    "            \n",
    "            if save_db:\n",
    "                cat.save_into_db()\n",
    "            \n",
    "            \n",
    "        \n",
    "    except Exception as err:\n",
    "        print('ERROR BY GET SUB CATEGORIES:', err)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = main_cats[0]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_all_categories() given a list of main categories (This is a recusion function)\n",
    "def get_all_categories(categories):\n",
    "    if len(categories) == 0:\n",
    "        return\n",
    "    \n",
    "    for cat  in categories:\n",
    "        sub_cat = get_sub_categories(cat, save_db = True)\n",
    "        print (sub_cat)\n",
    "        get_all_categories(sub_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_categories(main_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute('SELECT * FROM categories').fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "cat_table = pd.read_sql_query('SELECT * FROM categories', conn)\n",
    "cat_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_layer = pd.read_sql_query('SELECT table1.id, table1.name, table1.url FROM categories AS table1 LEFT JOIN categories AS table2 ON table1.id = table2.parent_id WHERE table2.parent_id IS NULL',conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_layer.to_csv(\"./links.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape all the products from these above links "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of all the links\n",
    "links = list(Final_layer['url'])\n",
    "links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_tiki_all(url = links):\n",
    "    data = []\n",
    "    for url in links:\n",
    "\n",
    "        url_base = url\n",
    "        page = 1\n",
    "\n",
    "        items = True\n",
    "        while items != [] and page <2:\n",
    "            try:\n",
    "                \n",
    "                soup = get_url(url)        \n",
    "                items = soup.find_all('div',{\"class\": \"product-item\"}) \n",
    "\n",
    "                for item in items: \n",
    "                    try: \n",
    "\n",
    "                        # Each tag is dictionary containing the required information\n",
    "                        dic = {\"product_id\":\"\",\"category\":\"\",\"seller_id\":\"\",\"title\":\"\",\"price\":\"\",\"image_url\":\"\"}\n",
    "\n",
    "                        dic['category'] = item['data-category'].split(\"/\")[0]\n",
    "                        dic[\"product_id\"] = item[\"data-id\"]\n",
    "                        dic[\"seller_id\"] = item[\"data-seller-product-id\"]\n",
    "                        dic[\"title\"] = item[\"data-title\"]\n",
    "                        dic[\"price\"] = item[\"data-price\"]\n",
    "\n",
    "                        # There are some items without img tag...\n",
    "                        if item.find(\"span\",{\"class\":\"image\"}):\n",
    "                            dic[\"image_url\"] = item.find(\"span\",{\"class\":\"image\"}).img[\"src\"]\n",
    "\n",
    "                        # Append the dictionary to data list\n",
    "                        data.append(dic)\n",
    "                    except:\n",
    "\n",
    "                         # Skip if error and print error message\n",
    "                        print(\"We got an error\")\n",
    "                # print out the page number and items to keep track\n",
    "                print(page, len(data))\n",
    "            \n",
    "            except:\n",
    "                print(\"er\")\n",
    "            # increment page\n",
    "            page += 1\n",
    "\n",
    "            # create the url of the next page\n",
    "            url = url_base + \"&page=\" +str(page)\n",
    "\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_products = scrape_tiki_all(url = links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table categories in the database using a function\n",
    "def create_products_table():\n",
    "    query = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS products (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            product_ID VARCHAR(255),\n",
    "            Seller_ID VARCHAR(255),\n",
    "            name VARCHAR(255),\n",
    "            price VARCHAR(255),\n",
    "            category TEXT,\n",
    "            image_url TEXT, \n",
    "            create_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        )\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cur.execute(query) # execute the query\n",
    "        conn.commit() # make changes\n",
    "    except Exception as err: \n",
    "        print('ERROR BY CREATE TABLE', err)\n",
    "        \n",
    "create_products_table()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute('DROP TABLE categories;')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute('SELECT * FROM products').fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = Product(\"1233\",\"ABS\",\"Huah\",\"819197\",\"Trang Suc\",\"link1\") #testing\n",
    "cur.execute('SELECT * FROM products').fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_tiki_all(url = links, save_db = False):\n",
    "    data = []\n",
    "    for url in links:\n",
    "\n",
    "        url_base = url\n",
    "        page = 1\n",
    "\n",
    "        items = True\n",
    "        while items != [] and page <2:\n",
    "            try:\n",
    "                \n",
    "                soup = get_url(url)        \n",
    "                items = soup.find_all('div',{\"class\": \"product-item\"}) \n",
    "\n",
    "                for item in items: \n",
    "                    try: \n",
    "\n",
    "                        # Each tag is dictionary containing the required information\n",
    "                        \n",
    "\n",
    "                        category = item['data-category'].split(\"/\")[0]\n",
    "                        product_id = item[\"data-id\"]\n",
    "                        seller_id = item[\"data-seller-product-id\"]\n",
    "                        name = item[\"data-title\"]\n",
    "                        price = item[\"data-price\"]\n",
    "                        \n",
    "\n",
    "                        # There are some items without img tag...\n",
    "                        if item.find(\"span\",{\"class\":\"image\"}):\n",
    "                            img = item.find(\"span\",{\"class\":\"image\"}).img[\"src\"]\n",
    "                        product = Product(product_id,seller_id,name,price,category,img)\n",
    "\n",
    "                        # Append the dictionary to data list\n",
    "                        data.append(product)\n",
    "                        \n",
    "                        if save_db:\n",
    "                            product.save_into_db()\n",
    "                    except:\n",
    "\n",
    "                         # Skip if error and print error message\n",
    "                        print(\"We got an error\")\n",
    "                # print out the page number and items to keep track\n",
    "                print(len(data))\n",
    "            \n",
    "            except:\n",
    "                print(\"IncompleteRead\")\n",
    "            # increment page\n",
    "            page += 1\n",
    "\n",
    "            # create the url of the next page\n",
    "            url = url_base + \"&page=\" +str(page)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_tiki_all(save_db = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_table = pd.read_sql_query('SELECT * FROM products', conn)\n",
    "product_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
